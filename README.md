# RAG-Based Conversational AI Infrastructure

## Overview

This project provides an AI-powered chatbot backend infrastructure using Retrieval-Augmented Generation (RAG). It supports document ingestion, vector embeddings, and natural language querying to create intelligent conversational experiences.

### Key Capabilities
- ğŸ“„ Document ingestion (PDFs)
- ğŸ” Vector embeddings using `all-mpnet-base-v2`
- ğŸ’¾ Vector storage with Chroma
- ğŸ¤– Natural language querying with `llama3` LLM
- ğŸš€ REST API built with FastAPI
- ğŸ’» Client frontend using Streamlit

The system supports multi-turn conversations with conversation ID tracking to avoid redundant ingestion and provides efficient session management.

## âœ¨ Features

- **Document Processing**: Upload and ingest documents via REST API
- **Persistent Storage**: Vector store with Chroma for document embeddings
- **Smart Embeddings**: Sentence Transformers using `all-mpnet-base-v2` model
- **Advanced Generation**: `llama3` language model for contextual responses
- **Async Architecture**: Pipeline initialization and ingestion caching
- **Session Management**: Conversation tracking to avoid repeated ingestion
- **Comprehensive Logging**: Debug and trace capabilities
- **File Handling**: Robust upload and storage management

## ğŸ› ï¸ Technology Stack

| Component | Technology / Model |
|-----------|-------------------|
| **Backend Framework** | FastAPI |
| **Async Concurrency** | asyncio |
| **Vector Store** | Chroma |
| **Embedding Model** | all-mpnet-base-v2 |
| **Language Model (LLM)** | llama3 |
| **Data Validation** | Pydantic |
| **ASGI Server** | Uvicorn |
| **Client UI** | Streamlit |
| **HTTP Client** | Requests (in Streamlit) |

## ğŸ—ï¸ Architecture & Workflow

### 1. Document Ingestion
Uploaded documents (PDFs) are processed through the following pipeline:
- Documents are saved to local storage
- Content is extracted and chunked
- Embeddings are generated using the embedding model
- Vectors are stored in Chroma vector database

### 2. Pipeline Initialization
- Global RAG pipeline instance initialized asynchronously
- On-demand startup or initialization
- Prevents repeated ingestion through caching mechanisms

### 3. Chat Request Processing
- Each request includes conversation ID for session tracking
- Pipeline queries vector store for relevant context
- LLM generates contextual responses based on retrieved information

### 4. Ingestion Caching
- Results cached in memory to avoid redundant processing
- Session-based caching for same conversation ID
- Time-to-live (TTL) expiration for cache invalidation

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- Required dependencies (see installation steps)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd rag-conversational-ai
   ```

2. **Install dependencies**
   ```bash
   pip install fastapi uvicorn pydantic chromadb sentence-transformers streamlit requests
   ```

3. **Run the backend server**
   ```bash
   uvicorn main:app --reload
   ```

4. **Launch the Streamlit client**
   ```bash
   streamlit run streamlit_app.py
   ```

### Usage

1. Access the Streamlit interface in your browser (typically `http://localhost:8501`)
2. Upload PDF documents through the interface
3. Start conversing with your documents using natural language queries
4. The system will provide contextual responses based on the uploaded content

## ğŸ“ Project Structure

```
RAG_APP/
â”œâ”€â”€ venv/                          # Virtual environment
â”œâ”€â”€ rag-chatbot/
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ __pycache__/           # Python cache files
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â”œâ”€â”€ __pycache__/
â”‚       â”‚   â”œâ”€â”€ temp/
â”‚       â”‚   â”œâ”€â”€ chat.py            # Chat API endpoints
â”‚       â”‚   â”œâ”€â”€ pipeline_state.json # Pipeline state configuration
â”‚       â”‚   â””â”€â”€ routes.py          # API route definitions
â”‚       â”œâ”€â”€ backend1.temp/
â”‚       â”‚   â””â”€â”€ sampledoc.pdf     # Sample document
â”‚       â”œâ”€â”€ data/                  # Data storage directory
â”‚       â”œâ”€â”€ factory/
â”‚       â”‚   â”œâ”€â”€ __pycache__/
â”‚       â”‚   â””â”€â”€ rag_factory.py    # RAG pipeline factory
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â”œâ”€â”€ __pycache__/
â”‚       â”‚   â”œâ”€â”€ chunker.py        # Document chunking service
â”‚       â”‚   â”œâ”€â”€ embedder.py       # Embedding generation service
â”‚       â”‚   â”œâ”€â”€ ingest.py         # Document ingestion service
â”‚       â”‚   â”œâ”€â”€ llm.py            # Language model service
â”‚       â”‚   â”œâ”€â”€ loader.py         # Document loading service
â”‚       â”‚   â”œâ”€â”€ logger.py         # Logging utilities
â”‚       â”‚   â”œâ”€â”€ prompt.py         # Prompt templates
â”‚       â”‚   â”œâ”€â”€ retrieval.py      # Document retrieval service
â”‚       â”‚   â””â”€â”€ vector_store.py   # Vector database operations
â”‚       â”œâ”€â”€ vector_store/
â”‚       â”‚   â””â”€â”€ 66c07bcd-9ed6-4b05-b30e-da7a159b9780/
â”‚       â”‚       â””â”€â”€ chroma.sqlite3 # Chroma vector database
â”‚       â””â”€â”€ app.py                # Main FastAPI application
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py          # Streamlit frontend client
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitattributes               # Git attributes configuration
â””â”€â”€ .gitignore                   # Git ignore rules
```

## ğŸ”® Future Improvements

- [ ] **Persistent Caching**: Implement disk or database-based ingestion cache for true persistence across restarts
- [ ] **Intent Classification**: Add detection for ingestion commands within chat interface
- [ ] **Enhanced Session Tracking**: Integrate Redis or database for robust session management
- [ ] **Security Enhancements**: Implement secure file uploads and API endpoint protection
- [ ] **Multi-format Support**: Extend beyond PDFs to support various document formats
- [ ] **User Authentication**: Add user management and access control
- [ ] **Performance Monitoring**: Implement metrics and performance tracking
- [ ] **Scalability**: Add support for distributed processing and load balancing

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

If you encounter any issues or have questions, please open an issue in the GitHub repository or contact the development team.

---

*Built with â¤ï¸ using FastAPI, Streamlit, and state-of-the-art AI technologies*